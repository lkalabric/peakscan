bad <- c()
bad_samples <- c()
for (i in sets_genotyped) {
# Extracts the correlation coeficiente between the ladder peak positions and sizes from the "list.data.covarrubia"
corro[[i]] <- unlist(lapply(list_data_allcovarrubias[[i]],function(x){x$corr}))
# Filter files with bad ladder
bad[[i]] <- which(corro[[i]] < .9999)
# Extract the file names from the list bad to the vector bad_data and print the results, if any
bad_samples[[i]] <- names(bad[[i]])
bad_samples[[i]]
readline(prompt = "Press [enter] to continue.")
}
View(corro)
View(bad)
length(bad_samples)
length(bad_samples[[1]])
bad_samples[[i]] == 0
bad_samples[[1]] == 0
length(bad_samples[[1]]) == 0
corro <- list()
bad <- c()
bad_samples <- c()
for (i in sets_genotyped) {
# Extracts the correlation coeficiente between the ladder peak positions and sizes from the "list.data.covarrubia"
print(paste("Testing ladder peaks from samples of",i))
corro[[i]] <- unlist(lapply(list_data_allcovarrubias[[i]],function(x){x$corr}))
# Filter files with bad ladder
bad[[i]] <- which(corro[[i]] < .9999)
# Extract the file names from the list bad to the vector bad_data and print the results, if any
bad_samples[[i]] <- names(bad[[i]])
if (lenght(bad_samples[[i]]) == 0) {
print("All samples have a good ladder!")
} else {
print(paste("Here are the bad samples:",bad_samples[[i]]))
}
readline(prompt = "Press [enter] to continue.")
}
corro <- list()
bad <- c()
bad_samples <- c()
for (i in sets_genotyped) {
# Extracts the correlation coeficiente between the ladder peak positions and sizes from the "list.data.covarrubia"
print(paste("Testing ladder peaks from samples of",i))
corro[[i]] <- unlist(lapply(list_data_allcovarrubias[[i]],function(x){x$corr}))
# Filter files with bad ladder
bad[[i]] <- which(corro[[i]] < .9999)
# Extract the file names from the list bad to the vector bad_data and print the results, if any
bad_samples[[i]] <- names(bad[[i]])
if (length(bad_samples[[i]]) == 0) {
print("All samples have a good ladder!")
} else {
print(paste("Here are the bad samples:",bad_samples[[i]]))
}
readline(prompt = "Press [enter] to continue.")
}
my_marker_sets
my_marker_sets[my_marker_sets$set == "set1"]
my_marker_sets$set == "set1"
expression(my_marker_sets$set == "set1")
subset(my_marker_sets, set == "set1")
i
maker_i <- subset(my_marker_sets, set == "set1")
View(maker_i)
i
makers_info = paste0("markers_",i)
markers_info = paste0("markers_",i)
markers_info
assign(markers_info,0)
markers_set2
assign(markers_info, data.frame())
markers_set2
typeof(markers_set2)
i
assign(markers_info, subset(my_marker_sets, set == i))
markers_set2
makers_info
markers_info$marker
markers_info <- subset(my_marker_sets, set == i)
markers_info$marker
markers_info$"smms3"$ch
View(markers_info)
markers_info$set
markers_info$marker["smms3"]
markers_info$marker == "smms3"
df[markers_info$marker == "smms3"]
df[markers_info$marker == "smms3",]
markers_info[markers_info$marker == "smms3",]
markers_info[markers_info$marker == "smms3"]
markers_info[markers_info$marker == "smms3",]
markers_info[markers_info$marker == "smms3",]$ch
markers_info[markers_info$marker == "smms3",]$x_min
fsa_alldata[[i]]
# General loop per batch and marker
# =========================================================
for (i in sets_genotyped) {
#markers_set_name = paste0("markers_",i)
#assign(markers_info, subset(my_marker_sets, set == i))
print(paste("Creating panel from markers of",i,"..."))
markers_info <- subset(my_marker_sets, set == i)
for (j in markers_info$marker) {
print("Working on marker",j,"...")
# Extract values from marker_info data.frame by marker
ch = markers_info[markers_info$marker == j,]$ch
x_min = markers_info[markers_info$marker == j,]$x_min
x_max = markers_info[markers_info$marker == j,]$x_max
my_panel <- overview2(my.inds=fsa_alldata[[i]], channel=ch, ladder=gs600liz, init.thresh=7000, xlim=c(x_min,x_max))
marker_name = paste0("marker_",i,"_",j)
assign(marker_name, my_painel)
print("Done.")
readline(prompt = "Press [enter] to continue.")
}
}
View(maker_i)
View(maker_i)
i
markers_info <- subset(my_marker_sets, set == i)
View(markers_info)
View(maker_i)
markers_info$marker
# General loop per batch and marker
# =========================================================
for (i in sets_genotyped) {
#markers_set_name = paste0("markers_",i)
#assign(markers_info, subset(my_marker_sets, set == i))
print(paste("Creating panel from markers of",i,"..."))
markers_info <- subset(my_marker_sets, set == i)
for (j in markers_info$marker) {
print(paste("Working on marker",j,"..."))
# Extract values from marker_info data.frame by marker
ch = markers_info[markers_info$marker == j,]$ch
x_min = markers_info[markers_info$marker == j,]$x_min
x_max = markers_info[markers_info$marker == j,]$x_max
my_panel <- overview2(my.inds=fsa_alldata[[i]], channel=ch, ladder=gs600liz, init.thresh=7000, xlim=c(x_min,x_max))
marker_name = paste0("marker_",i,"_",j)
assign(marker_name, my_painel)
print("Done.")
readline(prompt = "Press [enter] to continue.")
}
}
markers_info
ch
x_min
x_max
# General loop per batch and marker
# =========================================================
for (i in sets_genotyped) {
#markers_set_name = paste0("markers_",i)
#assign(markers_info, subset(my_marker_sets, set == i))
print(paste("Creating panel from markers of",i,"..."))
markers_info <- subset(my_marker_sets, set == i)
for (j in markers_info$marker) {
print(paste("Working on marker",j,"..."))
# Extract values from marker_info data.frame by marker
ch = markers_info[markers_info$marker == j,]$ch
x_min = markers_info[markers_info$marker == j,]$x_min
x_max = markers_info[markers_info$marker == j,]$x_max
my_panel <- overview2(my.inds=fsa_alldata[[i]], channel=ch, ladder=gs600liz, init.thresh=1000, xlim=c(x_min,x_max))
marker_name = paste0("marker_",i,"_",j)
assign(marker_name, my_painel)
print("Done.")
readline(prompt = "Press [enter] to continue.")
}
}
folder <- "~/myfolder"
folder <- "~/GitHub/peakscan/myfolder"
?my.plants
data(my.plants)
my.plants <- my.plants[1:2]
class(my.plants) <- "fsa_stored"
View(my.plants)
plot(my.plants)
plot(my.plants)
plot(my.plants)
plot(my.plants)
plot(my.plants)
my.ladder <- c(50, 75, 100, 125, 129, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375)
ladder.info.attach(stored=my.plants, ladder=my.ladder)
data(my.plants)
my.plants <- my.plants[1:2]
class(my.plants) <- "fsa_stored"
?my.plants
data(my.plants)
my.plants <- my.plants[1:2]
class(my.plants) <- "fsa_stored"
my.ladder <- c(50, 75, 100, 125, 129, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375)
ladder.info.attach(stored=my.plants, ladder=my.ladder)
my.ladder <- c(50, 75, 100, 125, 129, 150, 175, 200, 225, 250, 275, 300, 325, 350, 375)
ladder.info.attach(stored=my.plants, ladder=my.ladder)
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/peak-analysis") #Same location for github
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/peakscan/fsa" #has to be in its own separate folder
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
fsa_info <- get_fsa_metadata(fsa_dir) #Retrieve metadata about fsa files
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/peakscan/fsa_set1" #has to be in its own separate folder
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/peak-analysis") #Same location for github
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/peakscan/fsa_set1" #has to be in its own separate folder
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
fsa_info <- get_fsa_metadata(fsa_dir) #Retrieve metadata about fsa files
head(fsa_info)
write.table(fsa_info, file = paste0("fsa_info.txt"), sep = "\t", quote = FALSE, col.names = TRUE, row.names = FALSE)# Export metadata as tsv to working directory
#Batch import and extract all fsa files into one object
fsa_data <- storing_inds_rev3(fsa_dir, channels = 5, rawPlot = TRUE, fourier = TRUE, saturated = TRUE, lets.pullup = FALSE) #Import files, show plots
#Rename channels with associated dye names
fsa_data <- associate_dye_names(fsa_data, fsa_dir)#Association of dye names using import object and input directory
head(fsa_data[[1]], 5)
#Match the sizing ladder
GS600LIZ <- c(20, 40, 60, 80, 100, 114, 120, 140, 160, 180, 200, 214, 220, 240, 250, 260, 280, 300, 314, 320, 340, 360, 380, 400, 414, 420, 440, 460, 480, 500, 514, 520, 540, 560, 580, 600) #Define vector of internal standard/ladder sizes
ladder.info.attach(stored = fsa_data,ladder = GS600LIZ, ladd.init.thresh = 200, prog = FALSE, draw = TRUE) #Correlate internal ladder with samples
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/peak-analysis") #Same location for github
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/peakscan/fsa" #has to be in its own separate folder
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/peak-analysis") #Same location for github
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/peakscan/fsa" #has to be in its own separate folder
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
fsa_info <- get_fsa_metadata(fsa_dir) #Retrieve metadata about fsa files
knitr::opts_chunk$set(echo = TRUE) # This a global statment
# Note:
# ===============================================================
# `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the outputs and plots
# General advises:
# ===============================================================
# 1) use "_" instead of "." in variable and function names;
# 2) separate each block of code with a comment and "=====" whenever possible
# 3) this list will continue growing as soon as I learn more about programming in R...
# This piece of code turned out not to be useful since RStudio recognizes missing packages and asks for installation
# ===============================================================
list.of.packages <- c("Fragman","pacman")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loads package Fragman
# ===============================================================
library('Fragman')
# Loads other source codes
# ===============================================================
source("~/GitHub/peakscan/ladders_info.R") # ladders sizes
source("~/GitHub/peakscan/markers_info.R") # microsatellites sets
# General variables
# ===============================================================
threshold=100
my_sets <- unique(my_marker_sets$set)
# It is a good practice to store all fsa files in a different folder. # Read just one folder at a time
#fsa_dir <- "~/GitHub/peakscan/fsa_set1"
#fsa_data <- storing.inds(fsa_dir, channels=5, lets.pullup=FALSE)
# Maybe we should think about saving each set of files in a different folder (i.e. fsa_set1_dir, fsa_set2_dir, etc.)
# ===============================================================
sets_genotyped <- c()
fsa_alldata <- list()
for (i in my_sets) {
fsa_dir <- paste0("~/GitHub/peakscan/fsa_",i)
if (length(list.files(fsa_dir, all.files = TRUE, include.dirs = TRUE, no.. = TRUE)) > 0) {
list.files(fsa_dir, all.files = TRUE, include.dirs = TRUE, no.. = TRUE)
# Reads the fsa files and stores them within a list structure
# ===============================================================
print(i)
fsa_stored <- storing.inds(fsa_dir, channels=5, lets.pullup=FALSE)
# Include here all step of Fragman instead of accumulate to analyze later!
sets_genotyped <- c(sets_genotyped, i)
fsa_alldata <- c(fsa_alldata, list(fsa_stored))
} else {
print(paste("Directory",fsa_dir,"is empty!"))
}
}
names(fsa_alldata) <- sets_genotyped
# Create the list "list.data.covarrubias" indicating the ladder peaks position, height, weigth, correlation and error identified in each file
# ===============================================================
#ladder.info.attach(stored=fsa_data, ladder=gs600liz, ladd.init.thresh=threshold, prog=FALSE, draw=TRUE)
list_data_allcovarrubias <- list()
for (i in sets_genotyped) {
print(paste("Matching ladder from samples of",i,"..."))
ladder.info.attach(stored=fsa_alldata[[i]], ladder=gs600liz, ladd.init.thresh=threshold, prog=FALSE, draw=TRUE)
list_data_allcovarrubias <- c(list_data_allcovarrubias, list(list.data.covarrubias))
}
View(fsa_alldata)
View(fsa_alldata)
View(my_marker_sets)
View(my_marker_sets)
knitr::opts_chunk$set(echo = TRUE)
#Setting the working directory where scripts and data are found and calling the scripts
setwd("~/Documents/GitHub/P3_Pipeline/")
knitr::opts_chunk$set(echo = TRUE)
#Setting the working directory where scripts and data are found and calling the scripts
setwd("~/GitHub/P3_Pipeline/")
source("P3 Data manipulation Combo.R") #Data import scripts
source("JostD_KK.R")
source("Duplicate_check_KK.R")
knitr::opts_chunk$set(echo = TRUE)
inputPanel(
selectInput("n_breaks", label = "Number of bins:",
choices = c(10, 20, 35, 50), selected = 20),
sliderInput("bw_adjust", label = "Bandwidth adjustment:",
min = 0.2, max = 2, value = 1, step = 0.2)
)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
install.packages("remotes")
library("remotes")
install_gitlab("calvinw/ipynbdocument")
install.packages("remotes")
remotes::install_gitlab("calvinw/ipynbdocument")
install.packages("remotes")
knitr::opts_chunk$set(echo = TRUE)
install.packages("remotes")
remotes::install_gitlab("calvinw/ipynbdocument")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
install.packages("remotes")
library("remotes")
install_gitlab("calvinw/ipynbdocument")
install.packages("knitr")
install.packages("rmarkdown")
install.packages("tinytex")
tinytex::install_tinytex()
knitr::opts_chunk$set(echo = TRUE)
install.packages("remotes")
library("remotes")
install_gitlab("calvinw/ipynbdocument")
knitr::opts_chunk$set(echo = TRUE)
install.packages("remotes")
install.packages("remotes")
library(remotes)
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("adegenet", "cli", "cluster", "commonmark", "cpp11", "crayon", "curl", "evaluate", "foreign", "isoband", "jsonlite", "lifecycle", "MASS", "Matrix", "nlme", "nnet", "purrr", "qpdf", "readr", "rlang", "rmarkdown", "survival", "tidyselect", "tinytex", "vctrs", "vegan", "vroom"))
install.packages(c("cli", "rlang"))
install.packages(c("cli", "rlang"))
install.packages(c("cli", "rlang"))
install.packages(c("cli", "rlang"))
install.packages(c("cli", "rlang"))
install.packages(c("cli", "rlang"))
knitr::opts_chunk$set(echo = TRUE)
install.packages("knitr")
install.packages("rmarkdown")
knitr::opts_chunk$set(echo = TRUE)
install.packages("reticulate")
install.packages("remotes")
remotes::install_gitlab("calvinw/ipynbdocument")
x<-3
x
knitr::opts_chunk$set(echo = TRUE)
## Installing package ipynbdocument
install.packages("remotes")
remotes::install_gitlab("calvinw/ipynbdocument")
library(remotes)
source("~/GitHub/peakscan/markers_info.R", echo=TRUE)
View(my_marker_sets)
View(my_marker_sets)
View(my_marker_sets)
a <- my_marker_sets
a
---
title: "MS Genotyping pipeline"
# This piece of code turned out not to be useful since RStudio recognizes missing packages and asks for installation
# ===============================================================
list.of.packages <- c("Fragman","pacman")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loads package Fragman
# ===============================================================
library('Fragman')
# Loads other source codes
# ===============================================================
source("~/GitHub/peakscan/ladders_info.R") 		# ladders sizes
source("~/GitHub/peakscan/markers_info.R") 		# microsatellites info & sets (or batches)
source("~/GitHub/peakscan/research_data.R")		# epi & lab data
knitr::opts_chunk$set(echo = TRUE) # This a global statment
# General advises:
# ===============================================================
# 1) use "_" instead of "." in variable and function names;
# 2) separate each block of code with a comment and "=====" whenever possible
# 3) this list will continue growing as soon as I learn more about programming in R...
## MS Genotyping Pipeline
This is an effort to parse microsatellite trace files in fsa format together with epidemiological data to create useful data to be uploaded and analyzed at Online Program SpadeR https://chao.shinyapps.io/SpadeR/.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
## Setup
Required packages installation/loading alongside other source codes
```{r setup}
# This piece of code turned out not to be useful since RStudio recognizes missing packages and asks for installation
# ===============================================================
list.of.packages <- c("Fragman","pacman")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loads package Fragman
# ===============================================================
library('Fragman')
# Loads other source codes
# ===============================================================
source("~/GitHub/peakscan/ladders_info.R") 		# ladders sizes
source("~/GitHub/peakscan/markers_info.R") 		# microsatellites info & sets (or batches)
source("~/GitHub/peakscan/research_data.R")		# epi & lab data
# General variables
# ===============================================================
threshold=100
my_sets <- unique(my_marker_sets$set)
# Output data to Streamlit
write.csv(my_sets, "data/my_sets.csv", row.names = FALSE)
# Output data to Streamlit
write.csv(my_sets, "my_sets.csv", row.names = FALSE)
# Output data to Streamlit
write.csv(my_sets, "data/my_sets.csv", row.names = FALSE, col.names = FALSE)
write.csv(my_marker_sets, "data/my_marker_sets.csv", row.names = FALSE, col.names = FALSE)
View(my_marker_sets)
write.csv(my_marker_sets, "data/my_marker_sets.csv", row.names = FALSE)
names(my_sets) <- "set"
# Output data to Streamlit
write.csv(my_sets, "data/my_sets.csv", row.names = FALSE, col.names = FALSE)
my_sets.set
my_sets
my_sets <- unique(my_marker_sets$set)
my_sets
# Output data to Streamlit
write.csv(my_sets, "data/my_sets.csv")
write.csv(my_marker_sets, "data/my_marker_sets.csv")
