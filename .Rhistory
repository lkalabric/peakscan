# It is a good practice to store all fsa files in a different folder. Maybe we should think about saving each set of files in a different folder (i.e. fsa_set1_dir, fsa_set2_dir, etc.)
# ===============================================================
fsa_dir <- "~/GitHub/peakscan/fsa"
# Reads the fsa files and stores them within a list structure
# ===============================================================
fsa_data <- storing.inds(fsa_dir, channels=5, lets.pullup=FALSE)
# Create the list "list.data.covarrubias" indicating the ladder peaks position, height, weigth, correlation and error identified in each file
# ===============================================================
ladder.info.attach(stored=fsa_data, ladder=gs600liz, ladd.init.thresh=threshold, prog=FALSE, draw=TRUE)
### Matching your ladder is a critical step and should only happen once per batch of samples read
my.panel <- overview2(my.inds=fsa_data, cols=5, ladder=gs600liz, init.thresh=7000, xlim=c(160,190))
# Create the list "list.data.covarrubias" indicating the ladder peaks position, height, weigth, correlation and error identified in each file
# ===============================================================
ladder.info.attach(stored=fsa_data, ladder=gs600liz, ladd.init.thresh=threshold, prog=FALSE, draw=TRUE)
### Matching your ladder is a critical step and should only happen once per batch of samples read
my.panel <- overview2(my.inds=fsa_data, channel=5, ladder=gs600liz, init.thresh=7000, xlim=c(160,190))
# Extracts the correlation coeficiente between the ladder peak positions and sizes from the "list.data.covarrubia"
corro <- unlist(lapply(list.data.covarrubias,function(x){x$corr}))
# Filter files with bad ladder
bad <- which(corro < .9999)
# Extract the file names from the list bad to the vector bad_data and print the results, if any
bad_data <- names(bad)
bad_data
ladder.corrector(fsa_data, bad_data, gs600liz, thresh=threshold)
# Design panels for smms set 1
# =========================================================
smms2_panel <- overview2(my.inds=fsa_data, channel=1, ladder=gs600liz, init.thresh=7000, xlim=c(219,295))
smms2_panel <- locator(type="p", pch=20, col="blue")$x
a <- score.markers (my.inds=fsa_data, channel=1, panel=smms2_panel, ladder=gs600liz, electro=FALSE)
smms13_panel <- overview2(my.inds=fsa_data, channel=4, ladder=gs600liz, init.thresh=7000, xlim=c(171,228))
smms13_panel <- locator(type="p", pch=20, col="red")$x
a <- score.markers (my.inds=fsa_data, channel=4, panel=smms13_panel, ladder=gs600liz, electro=FALSE)
smms16_panel <- overview2(my.inds=fsa_data, channel=2, ladder=gs600liz, init.thresh=7000, xlim=c(201,258))
smms16_panel <- locator(type="p", pch=20, col="green")$x
a <- score.markers (my.inds=fsa_data, channel=2, panel=smms16_panel, ladder=gs600liz, electro=FALSE)
Sys.info()
if(.Platform$OS.type == "unix") {
} else {
}
if(.Platform$OS.type == "Windows") {
} else {
}
if(.Platform$OS.type == "windows") {
} else {
}
.Platform
if(.Platform$OS.type == "unix") { print ("TRUE")
} else {
print ("FALSE")
}
if(.Platform$OS.type == "windows") { print ("TRUE")
} else {
print ("FALSE")
}
tmp <- paste(getwd(),"tmp.xls",sep="/")
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
print(tmp)
.Platform$files.sep
.Platform$file.sep
print fsa_dir
print (fsa_dir)
gsub("/", "\\", fsa_dir, fixed=TRUE)
gsub("/", "\", fsa_dir, fixed=TRUE)
q
print (fsa_dir)
gsub("/", "\", fsa_dir, fixed=TRUE)
gsub("/", "\\", fsa_dir, fixed=TRUE)
rep <- gsub("/", "\\", fsa_dir, fixed=TRUE)
cat (rep)
knitr::opts_chunk$set(echo = TRUE)
#Import and prepare data for analysis in Fragman package
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
if(.Platform$OS.type == "windows") {
fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
if(.Platform$OS.type == "windows") {
fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
#Batch import and extract all fsa files into one object
fsa_data <- storing_inds_rev3(fsa_dir, channels = 5, rawPlot = TRUE, fourier = TRUE, saturated = TRUE, lets.pullup = FALSE) #Import files, show plots
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
#if(.Platform$OS.type == "windows") {
#  fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
#}
#Load JB Additional Scripts
setwd("~/Documents/GitHub/P3_Pipeline/")
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
#if(.Platform$OS.type == "windows") {
#  fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
#}
#Load JB Additional Scripts
setwd("~/Documents/GitHub/P3_Pipeline/")
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
#Batch import and extract all fsa files into one object
fsa_data <- storing_inds_rev3(fsa_dir, channels = 5, rawPlot = TRUE, fourier = TRUE, saturated = TRUE, lets.pullup = FALSE) #Import files, show plots
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/Documents/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
#if(.Platform$OS.type == "windows") {
#  fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
#}
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline/")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/Peak_Scoring_Output")
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
fsa_info <- get_fsa_metadata(fsa_dir) #Retrieve metadata about fsa files
#Batch import and extract all fsa files into one object
fsa_data <- storing_inds_rev3(fsa_dir, channels = 5, rawPlot = TRUE, fourier = TRUE, saturated = TRUE, lets.pullup = FALSE) #Import files, show plots
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
getwd()
knitr::opts_chunk$set(echo = TRUE)
###############################################################################
#                                                                             #
#                Introduction to Fragman peak scoring                         #
#                                                                             #
#   file:///Users/Kathleen/Downloads/fragman_intro_scoring.html               #
#                                                                             #
#    Written in R version 4.1.2 using R Studio RStudio 2022.02.0+443          #
#                                                                             #
#                                                                             #
###############################################################################
# Load required packages
library("Fragman") #1.0.9 - Peak scoring functions
library("tidyr") #1.1.4 - Tidyverse best practices
library("dplyr") #1.0.7 - Tidyverse best practices
library("magrittr") #2.0.1 - Use of L->R piping of functions
library("qpdf") #1.1 - PDF manipulations
#Designate the location of the fsa files to be analyzed
fsa_dir <- "~/GitHub/P3_Pipeline/fsa" #has to be in its own separate folder
#if(.Platform$OS.type == "windows") {
#  fsa_dir <- "~\Documents\GitHub\P3_Pipeline\fsa"
#}
#Load JB Additional Scripts
setwd("~/GitHub/P3_Pipeline/")
source("check_fsa_v_batch.R") #Data import scripts
source("get_fsa_metadata.R") #Data import scripts
source("storing_inds_rev3.R") #Data import scripts
source("associate_dye_names.R") #Data import scripts
source("score_markers_rev3.R") #Peak scoring scripts
source("Sm_mic_load_v3.R") #Schisto Microsatellite marker sets and expected sizes through Nov 2021
source("transform_scores_df.R") #Wrapper script for many small transformations and name cleanup (derived from J. Long)
#Change the working directory to where the analysis output should be saved
setwd("~/GitHub/P3_Pipeline/Peak_Scoring_Output")
#Import and prepare data for analysis in Fragman package
#Check formats and batch identifiers of all fsa files in target directory
check_fsa_v_batch(fsa_dir) #Summarize file formats and batch identifiers
fsa_info <- get_fsa_metadata(fsa_dir) #Retrieve metadata about fsa files
head(fsa_info)
write.table(fsa_info, file = paste0("fsa_info.txt"), sep = "\t", quote = FALSE, col.names = TRUE, row.names = FALSE)# Export metadata as tsv to working directory
#Batch import and extract all fsa files into one object
fsa_data <- storing_inds_rev3(fsa_dir, channels = 5, rawPlot = TRUE, fourier = TRUE, saturated = TRUE, lets.pullup = FALSE) #Import files, show plots
names(fsa_data) #Check sample names
head(fsa_data[[1]], 5) #Check data structure with first 5 lines of the first sample
#Rename channels with associated dye names
fsa_data <- associate_dye_names(fsa_data, fsa_dir)#Association of dye names using import object and input directory
head(fsa_data[[1]], 5)
View(fsa_data)
#Rename channels with associated dye names
fsa_data <- associate_dye_names(fsa_data, fsa_dir)#Association of dye names using import object and input directory
head(fsa_data[[1]], 5)
#Match the sizing ladder
GS600LIZ <- c(20, 40, 60, 80, 100, 114, 120, 140, 160, 180, 200, 214, 220, 240, 250, 260, 280, 300, 314, 320, 340, 360, 380, 400, 414, 420, 440, 460, 480, 500, 514, 520, 540, 560, 580, 600) #Define vector of internal standard/ladder sizes
ladder.info.attach(stored = fsa_data,ladder = GS600LIZ, ladd.init.thresh = 200, prog = FALSE, draw = TRUE) #Correlate internal ladder with samples
corro <- unlist(lapply(list.data.covarrubias, function(x){x$corr}))
bad <- which(corro < .999) #Check for samples with poorly correlated ladders
bad #View poorly correlated samples (if any)
#Rename channels with associated dye names
fsa_data <- associate_dye_names(fsa_data, fsa_dir)#Association of dye names using import object and input directory
head(fsa_data[[1]], 5)
#Match the sizing ladder
GS600LIZ <- c(20, 40, 60, 80, 100, 114, 120, 140, 160, 180, 200, 214, 220, 240, 250, 260, 280, 300, 314, 320, 340, 360, 380, 400, 414, 420, 440, 460, 480, 500, 514, 520, 540, 560, 580, 600) #Define vector of internal standard/ladder sizes
ladder.info.attach(stored = fsa_data,ladder = GS600LIZ, ladd.init.thresh = 200, prog = FALSE, draw = TRUE) #Correlate internal ladder with samples
corro <- unlist(lapply(list.data.covarrubias, function(x){x$corr}))
bad <- which(corro < .999) #Check for samples with poorly correlated ladders
bad #View poorly correlated samples (if any)
#Fragman fragment peak scoring
View(mic_set_list)
#Check microsatellite fragment panels
mic_29E6A #Examine panels of expected allele sizes for markers
`mic_SM13-478` #Examine panels of expected allele sizes for markers
mic_SMU31768 #Examine panels of expected allele sizes for markers
mic_15J15A #Examine panels of expected allele sizes for markers
mic_13TAGA
mic_1F8A
mic_L46951
mic_LG1_sc276
mic_LG3_sc36b
mic_LG5_sc475
mic_R95529
mic_sc23b
`mic_SM13-410`
mic_SMD28
mic_SMDA23
mic_SMMS13
mic_SMMS16
mic_SMMS17
mic_SMMS18
mic_SMMS2
mic_SMMS21
mic_SMMS3
#Score peaks for bin size and height
scores_29E6A <- score_markers_rev3(my.inds = fsa_data,
channel = 1,
# n.inds = "cwru.a_V_3_Sample_20201013_165003.fsa",
channel.ladder = 5,
panel = "mic_29E6A",
ladder = GS600LIZ,
init.thresh = 100,
ploidy = length(mic_29E6A),
windowL = 0.25,
windowR= 2,
left.cond = c(0, 2.5),
right.cond = 0,
pref = 1,
plotting = TRUE,
plotdir = "plots_scoring_intro") # Score markers for all, can also specify subset (n.inds); In pooled samples, the ploidy parameter = the number of expected fragment sizes
#This will output a pdf file in the place that was designated earlier under "plots_scoring_intro"
names(scores_29E6A) #The elements of the list object are scores for each sample
data.frame(scores_29E6A$cwru.a_V_3_Sample_20201013_165003.fsa) #Access a single element to see that sample's peak index positions, heights & weights
#Data manipulation per JB
#Transform Fragman list output to flatfiles
scores_29E6A_lf <- do.call(rbind.data.frame, scores_29E6A) #Convert list of sample scores to single long form data frame
scores_29E6A_lf$ID <- rownames(scores_29E6A_lf) #Add cleaned sample names
scores_29E6A_lf$ID <- gsub("_.*", "", scores_29E6A_lf$ID) #Add cleaned sample names
scores_29E6A_lf$filename <- rownames(scores_29E6A_lf) #Add cleaned file names
scores_29E6A_lf$filename <- gsub("\\.[0-9]*$", "", scores_29E6A_lf$filename) #Add cleaned file names
scores_29E6A_lf <- scores_29E6A_lf%>%distinct #Remove redundancies
scores_29E6A_tdf <- transform_scores_df(scores_29E6A) #Reshape and transpose output as per J. Long (L. Barbosa) for downstream analysis
head(scores_29E6A_lf, 5) #Print first 5 lines of longform data frame
head(scores_29E6A_tdf, 5) #Print first 5 lines of transformed data frame
#Export score flatfiles
write.table(scores_29E6A_lf, file = "scores_29E6A_lfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export long form data frame as tsv to working directory
write.table(scores_29E6A_tdf, file = "scores_29E6A_tdfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export transposed data frame as tsv to working directory
#Examine overview of peaks in data to estimate scoring parameters
ov_mic_SMU31768 <- overview2(my.inds = fsa_data,
channel = 1,
xlim = c(180, 232),
ylim = c(0, 5000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine channel 1 of ALL samples, focusing on the interval 150-180 bp.
ov_mic_SMU31768 #Show peak weights observed in the overview
mic_SMU31768 #Show expected fragments sizes (loaded at setup)
overview2(my.inds = fsa_data,
n.inds = which(names(fsa_data)%in%
c("cwru.a_V_3_Sample_20201013_165003.fsa")),
channel = 1,
xlim = c(180, 232),
ylim = c(0, 5000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine only the positive control
#Score peaks for bin size and height
scores_SMU31768 <- score_markers_rev3(my.inds = fsa_data,
channel = 1,
# n.inds = "cwru.a_V_3_Sample_20201013_165003.fsa",
channel.ladder = 5,
panel = "mic_SMU31768",
ladder = GS600LIZ,
init.thresh = 100,
ploidy = length(mic_SMU31768),
windowL = 0.25,
windowR= 2,
left.cond = c(0, 2.5),
right.cond = 0,
pref = 1,
plotting = TRUE,
plotdir = "plots_scoring_intro") # Score markers for all, can also specify subset (n.inds); In pooled samples, the ploidy parameter = the number of expected fragment sizes
#This will output a pdf file in the place that was designated earlier under "plots_scoring_intro"
names(scores_SMU31768) #The elements of the list object are scores for each sample
data.frame(scores_SMU31768$cwru.a_V_3_Sample_20201013_165003.fsa) #Access a single element to see that sample's peak index positions, heights & weights
#Data manipulation per JB
#Transform Fragman list output to flatfiles
scores_SMU31768_lf <- do.call(rbind.data.frame, scores_SMU31768) #Convert list of sample scores to single long form data frame
scores_SMU31768_lf$ID <- rownames(scores_SMU31768_lf) #Add cleaned sample names
scores_SMU31768_lf$ID <- gsub("_.*", "", scores_SMU31768_lf$ID) #Add cleaned sample names
scores_SMU31768_lf$filename <- rownames(scores_SMU31768_lf) #Add cleaned file names
scores_SMU31768_lf$filename <- gsub("\\.[0-9]*$", "", scores_SMU31768_lf$filename) #Add cleaned file names
scores_SMU31768_lf <- scores_SMU31768_lf%>%distinct #Remove redundancies
scores_SMU31768_tdf <- transform_scores_df(scores_SMU31768) #Reshape and transpose output as per J. Long (L. Barbosa) for downstream analysis
head(scores_SMU31768_lf, 5) #Print first 5 lines of longform data frame
head(scores_SMU31768_tdf, 5) #Print first 5 lines of transformed data frame
#Export score flatfiles
write.table(scores_SMU31768_lf, file = "scores_SMU31768_lfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export long form data frame as tsv to working directory
write.table(scores_SMU31768_tdf, file = "scores_SMU31768_tdfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export transposed data frame as tsv to working directory
#Examine overview of peaks in data to estimate scoring parameters
ov_mic_SM13478 <- overview2(my.inds = fsa_data,
channel = 1,
xlim = c(220, 260),
ylim = c(0, 8000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine channel 1 of ALL samples, focusing on the interval 150-180 bp.
ov_mic_SM13478 #Show peak weights observed in the overview
'mic_SM13-478' #Show expected fragments sizes (loaded at setup)
overview2(my.inds = fsa_data,
n.inds = which(names(fsa_data)%in%
c("cwru.a_V_3_Sample_20201013_165003.fsa")),
channel = 1,
xlim = c(220, 260),
ylim = c(0, 8000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine only the positive control
#Score peaks for bin size and height
scores_SM13478 <- score_markers_rev3(my.inds = fsa_data,
channel = 1,
# n.inds = "cwru.a_V_3_Sample_20201013_165003.fsa",
channel.ladder = 5,
panel = "mic_SM13-478",
ladder = GS600LIZ,
init.thresh = 100,
ploidy = length(`mic_SM13-478`),
windowL = 0.25,
windowR= 2,
left.cond = c(0, 2.5),
right.cond = 0,
pref = 1,
plotting = TRUE,
plotdir = "plots_scoring_intro") # Score markers for all, can also specify subset (n.inds); In pooled samples, the ploidy parameter = the number of expected fragment sizes
#This will output a pdf file in the place that was designated earlier under "plots_scoring_intro"
names(scores_SM13478) #The elements of the list object are scores for each sample
data.frame(scores_SM13478$cwru.a_V_3_Sample_20201013_165003.fsa) #Access a single element to see that sample's peak index positions, heights & weights
#Data manipulation per JB
#Transform Fragman list output to flatfiles
scores_SM13478_lf <- do.call(rbind.data.frame, scores_SM13478) #Convert list of sample scores to single long form data frame
scores_SM13478_lf$ID <- rownames(scores_SM13478_lf) #Add cleaned sample names
scores_SM13478_lf$ID <- gsub("_.*", "", scores_SM13478_lf$ID) #Add cleaned sample names
scores_SM13478_lf$filename <- rownames(scores_SM13478_lf) #Add cleaned file names
scores_SM13478_lf$filename <- gsub("\\.[0-9]*$", "", scores_SM13478_lf$filename) #Add cleaned file names
scores_SM13478_lf <- scores_SM13478_lf%>%distinct #Remove redundancies
scores_SM13478_tdf <- transform_scores_df(scores_SM13478) #Reshape and transpose output as per J. Long (L. Barbosa) for downstream analysis
head(scores_SM13478_lf, 5) #Print first 5 lines of longform data frame
head(scores_SM13478_tdf, 5) #Print first 5 lines of transformed data frame
#Export score flatfiles
write.table(scores_SM13478_lf, file = "scores_SM13478_lfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export long form data frame as tsv to working directory
write.table(scores_SM13478_tdf, file = "scores_SM13478_tdfex.txt", col.names = NA,
quote = FALSE, row.names = TRUE, sep = "\t") #Export transposed data frame as tsv to working directory
#Examine overview of peaks in data to estimate scoring parameters
ov_mic_15J15A <- overview2(my.inds = fsa_data,
channel = 1,
xlim = c(205, 235),
ylim = c(0, 5000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine channel 1 of ALL samples, focusing on the interval 150-180 bp.
ov_mic_15J15A #Show peak weights observed in the overview
mic_15J15A #Show expected fragments sizes (loaded at setup)
overview2(my.inds = fsa_data,
n.inds = which(names(fsa_data)%in%
c("cwru.a_V_3_Sample_20201013_165003.fsa")),
channel = 1,
xlim = c(205, 235),
ylim = c(0, 5000),
lwd = 2,
init.thresh = 100,
ladder = GS600LIZ,
verbose = FALSE)#Examine only the positive control
# Markers are multiplex for eletrophoresis by sets
set1 <- (name= c("smms2", "smms13", "smms16"))
set1_name <- (name= c("smms2", "smms13", "smms16"))
set1_ch <- (ch= c(1,4,2))
set1_x_min <- (x_min= c(219,171,201))
set1_x_max <- (x_max= c(295,228,258))
set1 <- data.frame(set1_name, set1_ch, set1_x_min, set1_x_max)
set1$set1_name
name <- (name= c("smms2", "smms13", "smms16"))
ch <- (ch= c(1,4,2))
x_min <- (x_min= c(219,171,201))
x_max <- (x_max= c(295,228,258))
set1 <- data.frame(name, ch, x_min, x_max)
set1$name
# This piece of code turned out not to be useful since RStudio recognizes missing packages and asks for installation
# ===============================================================
list.of.packages <- c("Fragman","pacman")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
# Loads package Fragman
# ===============================================================
library('Fragman')
# Loads other source codes
# ===============================================================
source("~/GitHub/peakscan/gs600liz_info.R") # GS600Liz ladder info
source("~/GitHub/peakscan/smms_info.R")     # S. mansoni microsatellite info
# General variables
# ===============================================================
threshold=200
for (i in 1..3) {
for (i in 1:3) {
i
var_name = paste0("set",i)
var_name
for (j in {$(var_name)} {
set1
for (i in 1:3) {
i
var_name = paste0("set",i)
var_name
for (j in set1} {
for (i in 1:3) {
i
var_name = paste0("set",i)
var_name
for (j in set1) {
j
}
}
x <- list(y = 1:10)
pass_by_value <- function(x){
x$y <- 10:1
}
pass_by_value(x)
x$y
var_name
